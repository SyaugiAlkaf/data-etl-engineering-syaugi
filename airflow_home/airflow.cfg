[core]
# The home folder for airflow, default is ~/airflow
airflow_home = D:/PyCharm Projects/pythonProject/dataEngineering/airflow_home

# The folder where your airflow pipelines live, most likely a
# subfolder in a code repository
# This path must be absolute
dags_folder = D:/PyCharm Projects/pythonProject/dataEngineering/airflow_home/dags

# The folder where airflow should store its log files
# This path must be absolute
base_log_folder = D:/PyCharm Projects/pythonProject/dataEngineering/airflow_home/logs

# Airflow can store logs remotely in AWS S3 or Google Cloud Storage. Users
# must supply an Airflow connection id that provides access to the storage
# location.
remote_base_log_folder =
remote_log_conn_id =
encrypt_s3_logs = False

# The executor class that airflow should use. Choices include
# SequentialExecutor, LocalExecutor, CeleryExecutor, DaskExecutor
# The default is SequentialExecutor
executor = SequentialExecutor

# The SqlAlchemy connection string to the metadata database.
# SqlAlchemy supports many different database engine, more information
# their documentation
sql_alchemy_conn = sqlite:///D:/PyCharm Projects/pythonProject/dataEngineering/airflow_home/airflow.db

# The SqlAlchemy pool size is the maximum number of database connections
# in the pool.
sql_alchemy_pool_size = 5

# The SqlAlchemy pool recycle is the number of seconds a connection
# can be idle in the pool before it is invalidated. A value of -1 means
# never recycle. Note that some database backends like MySQL have
# a default timeout of 8 hours, see
# https://dev.mysql.com/doc/refman/5.7/en/server-system-variables.html#sysvar_wait_timeout
sql_alchemy_pool_recycle = 1800

# The encoding for the databases
sql_alchemy_encoding = utf-8
